## Introduction

A toy project of JerryYin777

Reproduction Code of Paper "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention (MIT CSAIL)", [https://arxiv.org/pdf/2405.12981](https://arxiv.org/pdf/2405.12981)

- [x] Cross-Layer Attention
- [x] Cross-Layer Attention + MQA
- [ ] Cross-Layer Attention + GQA
- [ ] Cross-Layer Attention + MLA by DeepSeek(?)

The goal is to validate if I can implement CLA + MLA and achieve a SOTA reduction of KV Cache in a 1B to 3B model.
